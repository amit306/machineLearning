{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amit306/machineLearning/blob/main/00_Setup_and_Installation_COMPLETE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiZGLB-Jnvko"
      },
      "source": [
        "# Experiment 0: Setup and Installation\n",
        "### Model Cascading and Token-Aware Batching for LLM Serving\n",
        "\n",
        "**Purpose:** Install all required libraries and download datasets\n",
        "\n",
        "**Execution Time:** ~10-15 minutes\n",
        "\n",
        "**Run this notebook FIRST before all other experiments**\n",
        "\n",
        "**UPDATED VERSION:** Includes proper dataset caching for all experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYFLOJ9Tnvkp"
      },
      "source": [
        "## Step 1: Install Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "28eqh9winvkq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install core libraries\n",
        "!pip install transformers==4.36.0\n",
        "!pip install torch==2.1.0\n",
        "!pip install datasets==2.16.0\n",
        "!pip install scikit-learn==1.3.2\n",
        "!pip install accelerate==0.25.0\n",
        "!pip install bitsandbytes==0.41.3\n",
        "!pip install sentencepiece==0.1.99\n",
        "!pip install protobuf==3.20.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAuNyV3unvkr",
        "outputId": "0a3bd817-14e3-4456-fc35-7c6c916f0452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib==3.8.2 in /usr/local/lib/python3.12/dist-packages (3.8.2)\n",
            "Requirement already satisfied: seaborn==0.13.0 in /usr/local/lib/python3.12/dist-packages (0.13.0)\n",
            "Requirement already satisfied: pandas==2.1.4 in /usr/local/lib/python3.12/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy==1.26.3 in /usr/local/lib/python3.12/dist-packages (1.26.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.2) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.2) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.2) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.2) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.2) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.2) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.2) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib==3.8.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.12/dist-packages (from pandas==2.1.4) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib==3.8.2) (1.17.0)\n",
            "Requirement already satisfied: scipy==1.11.4 in /usr/local/lib/python3.12/dist-packages (1.11.4)\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.12/dist-packages (4.66.1)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.12/dist-packages (from scipy==1.11.4) (1.26.3)\n"
          ]
        }
      ],
      "source": [
        "# Install visualization and analysis libraries\n",
        "!pip install matplotlib==3.8.2 seaborn==0.13.0 pandas==2.1.4 numpy==1.26.3\n",
        "!pip install scipy==1.11.4 tqdm==4.66.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FASVG__xnvks",
        "outputId": "361cee5f-eb6a-44f8-9f2f-dfcfe69938ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simpy==4.1.1 in /usr/local/lib/python3.12/dist-packages (4.1.1)\n"
          ]
        }
      ],
      "source": [
        "# Install simulation library for batching experiments\n",
        "!pip install simpy==4.1.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWyyhx7gnvks"
      },
      "source": [
        "## Step 2: Verify Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBz9pm_2nvkt",
        "outputId": "e8a01387-d7f8-4c09-999d-1cfc0e52f0c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All core libraries imported successfully\n",
            "PyTorch version: 2.9.0+cpu\n",
            "Transformers version: 4.36.0\n",
            "Datasets version: 2.16.0\n",
            "Scikit-learn version: 1.3.2\n",
            "\n",
            "GPU Available: False\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import transformers\n",
        "import sklearn\n",
        "import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"✓ All core libraries imported successfully\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Datasets version: {datasets.__version__}\")\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "print(f\"\\nGPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gSinjrvnvkt"
      },
      "source": [
        "## Step 3: Download and Cache Datasets\n",
        "\n",
        "This step downloads all required datasets and caches them for use in experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aa0NbymInvku",
        "outputId": "dba347d4-e490-478c-cc08-a0ac76089232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading datasets... This may take a few minutes.\n",
            "\n",
            "1. Downloading AG News...\n",
            "   ✓ AG News: 120000 train, 7600 test samples\n",
            "\n",
            "2. Downloading TREC...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/datasets/load.py:1429: FutureWarning: The repository for trec contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/trec\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ TREC: 5452 train, 500 test samples\n",
            "\n",
            "3. Downloading SST-2...\n",
            "   ✓ SST-2: 67349 train, 872 validation samples\n",
            "\n",
            "4. Downloading 20 Newsgroups...\n",
            "   → Trying sklearn fetch_20newsgroups...\n",
            "   ✗ sklearn failed: HTTP Error 403: Forbidden\n",
            "   → Trying HuggingFace datasets...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ 20 Newsgroups (HuggingFace): 11314 train, 7532 test samples\n",
            "\n",
            "✓ All datasets downloaded successfully!\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "print(\"Downloading datasets... This may take a few minutes.\\n\")\n",
        "\n",
        "# Dataset 1: AG News (News Classification)\n",
        "print(\"1. Downloading AG News...\")\n",
        "ag_news = load_dataset(\"ag_news\")\n",
        "print(f\"   ✓ AG News: {len(ag_news['train'])} train, {len(ag_news['test'])} test samples\")\n",
        "\n",
        "# Dataset 2: TREC (Question Classification)\n",
        "print(\"\\n2. Downloading TREC...\")\n",
        "trec = load_dataset(\"trec\")\n",
        "print(f\"   ✓ TREC: {len(trec['train'])} train, {len(trec['test'])} test samples\")\n",
        "\n",
        "# Dataset 3: SST-2 (Sentiment Analysis)\n",
        "print(\"\\n3. Downloading SST-2...\")\n",
        "sst2 = load_dataset(\"glue\", \"sst2\")\n",
        "print(f\"   ✓ SST-2: {len(sst2['train'])} train, {len(sst2['validation'])} validation samples\")\n",
        "\n",
        "# Dataset 4: 20 Newsgroups (Multi-class Classification)\n",
        "print(\"\\n4. Downloading 20 Newsgroups...\")\n",
        "\n",
        "newsgroups_loaded = False\n",
        "\n",
        "# Try Method 1: sklearn (fastest if it works)\n",
        "try:\n",
        "    print(\"   → Trying sklearn fetch_20newsgroups...\")\n",
        "    newsgroups_train = fetch_20newsgroups(\n",
        "        subset='train',\n",
        "        remove=('headers', 'footers', 'quotes'),\n",
        "        random_state=42\n",
        "    )\n",
        "    newsgroups_test = fetch_20newsgroups(\n",
        "        subset='test',\n",
        "        remove=('headers', 'footers', 'quotes'),\n",
        "        random_state=42\n",
        "    )\n",
        "    print(f\"   ✓ 20 Newsgroups (sklearn): {len(newsgroups_train.data)} train, {len(newsgroups_test.data)} test samples\")\n",
        "    newsgroups_loaded = True\n",
        "except Exception as e:\n",
        "    print(f\"   ✗ sklearn failed: {e}\")\n",
        "    newsgroups_loaded = False\n",
        "\n",
        "# Try Method 2: HuggingFace (reliable fallback)\n",
        "if not newsgroups_loaded:\n",
        "    try:\n",
        "        print(\"   → Trying HuggingFace datasets...\")\n",
        "        hf_newsgroups = load_dataset(\"SetFit/20_newsgroups\")\n",
        "\n",
        "        # Create sklearn-compatible Bunch object\n",
        "        class NewsGroupsBunch:\n",
        "            def __init__(self, data, target, target_names=None):\n",
        "                self.data = data\n",
        "                self.target = target\n",
        "                self.target_names = target_names if target_names else list(range(max(target) + 1))\n",
        "\n",
        "        newsgroups_train = NewsGroupsBunch(\n",
        "            data=hf_newsgroups['train']['text'],\n",
        "            target=hf_newsgroups['train']['label']\n",
        "        )\n",
        "        newsgroups_test = NewsGroupsBunch(\n",
        "            data=hf_newsgroups['test']['text'],\n",
        "            target=hf_newsgroups['test']['label']\n",
        "        )\n",
        "\n",
        "        print(f\"   ✓ 20 Newsgroups (HuggingFace): {len(newsgroups_train.data)} train, {len(newsgroups_test.data)} test samples\")\n",
        "        newsgroups_loaded = True\n",
        "    except Exception as e:\n",
        "        print(f\"   ✗ HuggingFace failed: {e}\")\n",
        "        newsgroups_loaded = False\n",
        "\n",
        "if not newsgroups_loaded:\n",
        "    print(\"\\n   ⚠️  WARNING: Could not download 20 Newsgroups dataset\")\n",
        "    print(\"   Experiment 01 will not work without this dataset.\")\n",
        "    print(\"   Please check your internet connection and try again.\")\n",
        "else:\n",
        "    print(\"\\n✓ All datasets downloaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-yGPTiWnvkv"
      },
      "source": [
        "## Step 4: Cache Datasets for Experiments\n",
        "\n",
        "**CRITICAL STEP:** Save datasets to pickle files so experiments can load them quickly without re-downloading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfNVfnS_nvkv",
        "outputId": "ed38dfe9-3e78-4040-d1cb-7fba6883cc3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caching datasets for experiments...\n",
            "\n",
            "1. Caching 20 Newsgroups...\n",
            "   ✓ Saved 11314 train samples\n",
            "   ✓ Saved 7532 test samples\n",
            "   → Experiment 01 will use this cached version\n",
            "\n",
            "2. Caching AG News...\n",
            "   ✓ Saved 120000 train samples\n",
            "   ✓ Saved 7600 test samples\n",
            "   → Experiment 02 will use this cached version\n",
            "\n",
            "✓ All datasets cached successfully!\n",
            "   Location: experiment_results/\n",
            "   Files: newsgroups_train.pkl, newsgroups_test.pkl, ag_news.pkl\n"
          ]
        }
      ],
      "source": [
        "print(\"Caching datasets for experiments...\\n\")\n",
        "\n",
        "# Create experiment_results directory\n",
        "os.makedirs('experiment_results', exist_ok=True)\n",
        "\n",
        "# Cache 20 Newsgroups (for Experiment 01)\n",
        "if newsgroups_loaded:\n",
        "    print(\"1. Caching 20 Newsgroups...\")\n",
        "    with open('experiment_results/newsgroups_train.pkl', 'wb') as f:\n",
        "        pickle.dump(newsgroups_train, f)\n",
        "    with open('experiment_results/newsgroups_test.pkl', 'wb') as f:\n",
        "        pickle.dump(newsgroups_test, f)\n",
        "    print(f\"   ✓ Saved {len(newsgroups_train.data)} train samples\")\n",
        "    print(f\"   ✓ Saved {len(newsgroups_test.data)} test samples\")\n",
        "    print(\"   → Experiment 01 will use this cached version\")\n",
        "else:\n",
        "    print(\"1. ⚠️  Skipping 20 Newsgroups cache (download failed)\")\n",
        "\n",
        "# Cache AG News (for Experiment 02)\n",
        "print(\"\\n2. Caching AG News...\")\n",
        "ag_news_cache = {\n",
        "    'train': ag_news['train'],\n",
        "    'test': ag_news['test']\n",
        "}\n",
        "with open('experiment_results/ag_news.pkl', 'wb') as f:\n",
        "    pickle.dump(ag_news_cache, f)\n",
        "print(f\"   ✓ Saved {len(ag_news['train'])} train samples\")\n",
        "print(f\"   ✓ Saved {len(ag_news['test'])} test samples\")\n",
        "print(\"   → Experiment 02 will use this cached version\")\n",
        "\n",
        "print(\"\\n✓ All datasets cached successfully!\")\n",
        "print(\"   Location: experiment_results/\")\n",
        "print(\"   Files: newsgroups_train.pkl, newsgroups_test.pkl, ag_news.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cuDNaRSnvkw"
      },
      "source": [
        "## Step 5: Download and Cache Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n59GIdGNnvkw",
        "outputId": "8c51549a-803d-4dc8-d892-2fadd2610b18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading models... This will take several minutes.\n",
            "\n",
            "1. Downloading DistilBERT (small model)...\n",
            "   ✓ DistilBERT tokenizer cached\n",
            "\n",
            "2. Downloading TinyBERT (tiny model)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ✓ TinyBERT tokenizer cached\n",
            "\n",
            "3. Downloading FLAN-T5-base (LLM)...\n",
            "   ✓ FLAN-T5 tokenizer cached\n",
            "\n",
            "✓ All models downloaded and cached!\n",
            "   Models are cached in HuggingFace's default cache directory\n",
            "   They will load quickly in subsequent notebooks\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "print(\"Downloading models... This will take several minutes.\\n\")\n",
        "\n",
        "# Model 1: DistilBERT (small model for cascading)\n",
        "print(\"1. Downloading DistilBERT (small model)...\")\n",
        "distilbert_tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "print(\"   ✓ DistilBERT tokenizer cached\")\n",
        "\n",
        "# Model 2: TinyBERT (tiny model alternative)\n",
        "print(\"\\n2. Downloading TinyBERT (tiny model)...\")\n",
        "try:\n",
        "    tinybert_tokenizer = AutoTokenizer.from_pretrained('huawei-noah/TinyBERT_General_4L_312D')\n",
        "    print(\"   ✓ TinyBERT tokenizer cached\")\n",
        "except:\n",
        "    print(\"   ⚠️  TinyBERT unavailable (optional)\")\n",
        "\n",
        "# Model 3: FLAN-T5 (LLM for cascading)\n",
        "print(\"\\n3. Downloading FLAN-T5-base (LLM)...\")\n",
        "flant5_tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-base')\n",
        "print(\"   ✓ FLAN-T5 tokenizer cached\")\n",
        "\n",
        "print(\"\\n✓ All models downloaded and cached!\")\n",
        "print(\"   Models are cached in HuggingFace's default cache directory\")\n",
        "print(\"   They will load quickly in subsequent notebooks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4uWzD62nvkx"
      },
      "source": [
        "## Step 6: Create Directory Structure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "URBKQ60Mnvkx",
        "outputId": "03db9868-fe77-4a19-b79c-50d3409592fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating directory structure...\n",
            "\n",
            "✓ Created: experiment_results/\n",
            "✓ Created: models/\n",
            "✓ Created: figures/\n",
            "\n",
            "✓ Directory structure ready:\n",
            "  - experiment_results/ (for CSV results and cached data)\n",
            "  - models/ (for saving trained models)\n",
            "  - figures/ (for saving plots)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create all necessary directories\n",
        "directories = [\n",
        "    'experiment_results',\n",
        "    'models',\n",
        "    'figures'\n",
        "]\n",
        "\n",
        "print(\"Creating directory structure...\\n\")\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    print(f\"✓ Created: {directory}/\")\n",
        "\n",
        "print(\"\\n✓ Directory structure ready:\")\n",
        "print(\"  - experiment_results/ (for CSV results and cached data)\")\n",
        "print(\"  - models/ (for saving trained models)\")\n",
        "print(\"  - figures/ (for saving plots)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tFcQlf7nvkx"
      },
      "source": [
        "## Step 7: Save Experiment Metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLMRZslynvky",
        "outputId": "732ae7b2-21c8-4f42-a4f8-bd78adff61ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Experiment metadata saved to experiment_results/setup_metadata.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Save experiment metadata\n",
        "metadata = {\n",
        "    'torch_version': torch.__version__,\n",
        "    'transformers_version': transformers.__version__,\n",
        "    'datasets_version': datasets.__version__,\n",
        "    'sklearn_version': sklearn.__version__,\n",
        "    'gpu_available': torch.cuda.is_available(),\n",
        "    'datasets_downloaded': ['ag_news', 'trec', 'sst2', '20newsgroups'],\n",
        "    'models_cached': ['distilbert-base-uncased', 'flan-t5-base'],\n",
        "    'newsgroups_loaded': newsgroups_loaded,\n",
        "    'cache_location': 'experiment_results/'\n",
        "}\n",
        "\n",
        "with open('experiment_results/setup_metadata.pkl', 'wb') as f:\n",
        "    pickle.dump(metadata, f)\n",
        "\n",
        "print(\"✓ Experiment metadata saved to experiment_results/setup_metadata.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w3ABApUnvky"
      },
      "source": [
        "## Step 8: Test Basic Functionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYDi7D8dnvkz",
        "outputId": "ed9b0340-77e7-452a-e96f-fd1fd511ef4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing basic ML pipeline...\n",
            "\n",
            "✓ Test classification accuracy: 0.00%\n",
            "✓ Basic ML pipeline working correctly\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"Testing basic ML pipeline...\\n\")\n",
        "\n",
        "if newsgroups_loaded:\n",
        "    # Small test with 20 newsgroups\n",
        "    X_train_sample = newsgroups_train.data[:100]\n",
        "    y_train_sample = newsgroups_train.target[:100]\n",
        "    X_test_sample = newsgroups_test.data[:20]\n",
        "    y_test_sample = newsgroups_test.target[:20]\n",
        "\n",
        "    # Create simple TF-IDF + LogReg pipeline\n",
        "    vectorizer = TfidfVectorizer(max_features=1000)\n",
        "    X_train_vec = vectorizer.fit_transform(X_train_sample)\n",
        "    X_test_vec = vectorizer.transform(X_test_sample)\n",
        "\n",
        "    clf = LogisticRegression(max_iter=100, random_state=42)\n",
        "    clf.fit(X_train_vec, y_train_sample)\n",
        "\n",
        "    score = clf.score(X_test_vec, y_test_sample)\n",
        "    print(f\"✓ Test classification accuracy: {score:.2%}\")\n",
        "    print(\"✓ Basic ML pipeline working correctly\")\n",
        "else:\n",
        "    print(\"⚠️  Skipping ML test (20 Newsgroups not available)\")\n",
        "    print(\"   Experiment 01 will need this dataset to run\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NpRI9glhnvk0"
      },
      "source": [
        "## Step 9: Verify Cache Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2BLbrVZnvk0",
        "outputId": "599910b5-1f14-4172-9915-9b0c92052e23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Verifying cached files...\n",
            "\n",
            "✓ experiment_results/newsgroups_train.pkl (13.22 MB)\n",
            "✓ experiment_results/newsgroups_test.pkl (7.93 MB)\n",
            "✓ experiment_results/ag_news.pkl (0.00 MB)\n",
            "✓ experiment_results/setup_metadata.pkl (0.00 MB)\n",
            "\n",
            "✓ All cache files present and ready!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "print(\"Verifying cached files...\\n\")\n",
        "\n",
        "cache_files = [\n",
        "    'experiment_results/newsgroups_train.pkl',\n",
        "    'experiment_results/newsgroups_test.pkl',\n",
        "    'experiment_results/ag_news.pkl',\n",
        "    'experiment_results/setup_metadata.pkl'\n",
        "]\n",
        "\n",
        "all_present = True\n",
        "for filepath in cache_files:\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f\"✓ {filepath} ({size_mb:.2f} MB)\")\n",
        "    else:\n",
        "        print(f\"✗ {filepath} (MISSING)\")\n",
        "        all_present = False\n",
        "\n",
        "if all_present:\n",
        "    print(\"\\n✓ All cache files present and ready!\")\n",
        "else:\n",
        "    print(\"\\n⚠️  Some cache files are missing\")\n",
        "    print(\"   You may need to re-run the setup steps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5Yw__vvnvk1"
      },
      "source": [
        "## Step 10: System Information Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NDh9HrsHnvk1",
        "outputId": "15d2e6e0-74c0-4795-b7d6-168b6cd77e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "SYSTEM INFORMATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "Platform: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "Python Version: 3.12.12\n",
            "CPU Cores: 1 physical, 2 logical\n",
            "RAM: 13.6 GB total, 12.1 GB available\n",
            "\n",
            "GPU: No (CPU only)\n",
            "\n",
            "============================================================\n",
            "SETUP COMPLETE - READY FOR EXPERIMENTS\n",
            "============================================================\n",
            "\n",
            "Next steps:\n",
            "  ✓ Run Experiment 1: Model Cascading - Basic (Notebook 01)\n",
            "  ✓ Run Experiment 2: Model Cascading - Advanced (Notebook 02)\n",
            "  ✓ Run Experiment 3: Token-Aware Batching (Notebook 03)\n",
            "  ✓ Run Analysis: Results Visualization (Notebook 04)\n",
            "\n",
            "Cached datasets available in: experiment_results/\n",
            "All experiments will load from cache (fast!)\n"
          ]
        }
      ],
      "source": [
        "import platform\n",
        "import psutil\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SYSTEM INFORMATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nPlatform: {platform.platform()}\")\n",
        "print(f\"Python Version: {platform.python_version()}\")\n",
        "print(f\"CPU Cores: {psutil.cpu_count(logical=False)} physical, {psutil.cpu_count(logical=True)} logical\")\n",
        "print(f\"RAM: {psutil.virtual_memory().total / 1e9:.1f} GB total, {psutil.virtual_memory().available / 1e9:.1f} GB available\")\n",
        "\n",
        "print(f\"\\nGPU: {'Yes (' + torch.cuda.get_device_name(0) + ')' if torch.cuda.is_available() else 'No (CPU only)'}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SETUP COMPLETE - READY FOR EXPERIMENTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nNext steps:\")\n",
        "if newsgroups_loaded:\n",
        "    print(\"  ✓ Run Experiment 1: Model Cascading - Basic (Notebook 01)\")\n",
        "else:\n",
        "    print(\"  ⚠️  Experiment 1 requires 20 Newsgroups (download failed)\")\n",
        "    print(\"     Try re-running this setup notebook\")\n",
        "print(\"  ✓ Run Experiment 2: Model Cascading - Advanced (Notebook 02)\")\n",
        "print(\"  ✓ Run Experiment 3: Token-Aware Batching (Notebook 03)\")\n",
        "print(\"  ✓ Run Analysis: Results Visualization (Notebook 04)\")\n",
        "\n",
        "print(\"\\nCached datasets available in: experiment_results/\")\n",
        "print(\"All experiments will load from cache (fast!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YXXuo1Znvk1"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "If you encounter issues:\n",
        "\n",
        "1. **Out of Memory**: Restart runtime and run this notebook again\n",
        "2. **Download Failures**:\n",
        "   - Check internet connection\n",
        "   - Re-run the affected download cell\n",
        "   - The notebook has fallback methods (sklearn → HuggingFace)\n",
        "3. **Import Errors**: Ensure all cells in \"Step 1\" completed successfully\n",
        "4. **GPU Not Available**: This is fine - experiments will run on CPU (slower but functional)\n",
        "5. **20 Newsgroups Failed**:\n",
        "   - Both sklearn and HuggingFace failed\n",
        "   - Check internet connection\n",
        "   - Try restarting runtime and re-running setup\n",
        "   - Experiment 01 cannot run without this dataset\n",
        "   - Experiments 02-04 will still work\n",
        "\n",
        "**Important for Colab Users:**\n",
        "- Free Colab has ~12GB RAM and may disconnect after 12 hours\n",
        "- If disconnected, you'll need to re-run this setup notebook\n",
        "- Cache files persist in the session, so re-running is fast\n",
        "- Consider Colab Pro if you need more resources or longer sessions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}